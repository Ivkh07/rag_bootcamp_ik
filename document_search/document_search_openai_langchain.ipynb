{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# OpenAI Document Search with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [langchain](https://python.langchain.com/docs/get_started/introduction) library to run a text-generation request on open-source LLMs and embedding models using the OpenAI SDK, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "**Requirements:**\n",
    "- As you will accessing the LLMs and embedding models through Vector AI Engineering's Kaleidoscope Service (Vector Inference + Autoscaling), you will need to request a KScope API Key. Visit [https://kscope.vectorinstitute.ai/](https://kscope.vectorinstitute.ai/) and select \"Request API Key\" to get started.\n",
    "- After obtaining the `.env` configurations, paste the content into `~/.kscope.env`. This notebook will read these configs from that file.\n",
    "- (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "Make sure other necessary items are in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import re \n",
    "\n",
    "    for line in open(Path.home() / \".kscope.env\", \"r\").read().splitlines():\n",
    "        match = re.match(r\"^(export)? (\\w+)=\\\"?([^\\\"]+)\\\"?$\", line)\n",
    "        if match:\n",
    "            _, key, value = match.groups()\n",
    "            os.environ[key] = value\n",
    "        \n",
    "    assert os.environ.get(\"OPENAI_API_KEY\") is not None\n",
    "\n",
    "except Exception as err:\n",
    "    print(f\"Could not read your OpenAI key. Please make sure this is available in plain text under your home directory in ~/.kscope.env: {err}\")\n",
    "\n",
    "# Look for the source_documents folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")\n",
    "\n",
    "GENERATOR_BASE_URL = \"https://kscope.vectorinstitute.ai/v1\"\n",
    "EMBEDDING_BASE_URL = \"https://kscope.vectorinstitute.ai/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "\n",
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Please introduce yourself.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "client = openai.OpenAI(base_url=GENERATOR_BASE_URL, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# If model is not yet available, try again after some delay.\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = client.chat.completions.create(\n",
    "            model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=128,\n",
    "        )\n",
    "    \n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "\n",
    "print(json.dumps(output.model_dump()[\"choices\"], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558afb",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation\n",
    "\n",
    "Let's start by asking OpenAI a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's basic knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is very domain-specific that it won't know the answer to. A good example would an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "\"*How many Vector scholarships in AI were awarded in 2022?*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## Now send the query to KScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00061d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"Meta-Llama-3.1-8B-Instruct\", base_url=GENERATOR_BASE_URL, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "message = [\n",
    "    HumanMessage(\n",
    "        content=query\n",
    "    )\n",
    "]\n",
    "result = llm(message)\n",
    "print(f\"Result: \\n\\n{result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Cohere is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`, break them up into smaller digestible chunks, then encode them as vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdfs\n",
    "directory_path = \"./source_documents\"\n",
    "loader = PyPDFDirectoryLoader(directory_path)\n",
    "docs = loader.load()\n",
    "print(f\"Number of source documents: {len(docs)}\")\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Number of text chunks: {len(chunks)}\")\n",
    "\n",
    "# Define the embeddings model\n",
    "model_name = \"bge-multilingual-gemma2\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "print(f\"Setting up the embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings(base_url=EMBEDDING_BASE_URL, model=\"bge-multilingual-gemma2\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "print(f\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "# Retrieval: Make the document chunks available via a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc16fe",
   "metadata": {},
   "source": [
    "The retriever will identify the document chunks that most closely match our original query. (This takes about 1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# Retrieve the most relevant context from the vector store based on the query(No Reranking Applied)\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1690e",
   "metadata": {},
   "source": [
    "Let's see what results it found. Important to note, these results are in the order the retriever thought were the best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ede5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "These results seem to somewhat match our original query, but we still can't seem to find the information we're looking for. Let's try sending our LLM query again including these results, and see what it comes up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23499f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sending the RAG generation with query: {query}\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever)\n",
    "print(f\"Result:\\n\\n{qa.run(query=query)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea847fe",
   "metadata": {},
   "source": [
    "# Reranking: Improve the ordering of the document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(base_url=EMBEDDING_BASE_URL, model=\"bge-multilingual-gemma2\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, base_retriever=retriever\n",
    ")\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20a66b",
   "metadata": {},
   "source": [
    "Now let's see what the reranked results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961dda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef217bc",
   "metadata": {},
   "source": [
    "Lastly, let's run our LLM query a final time with the reranked results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63696ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=compression_retriever)\n",
    "\n",
    "print(f\"Result:\\n\\n {qa.run(query=query)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
