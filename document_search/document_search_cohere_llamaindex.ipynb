{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# Cohere Document Search with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request against [Cohere's](https://cohere.com/) API, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "**Requirements:**\n",
    "- You will need an access key to Cohere's API key, which you can sign up for at (https://dashboard.cohere.com/welcome/login). A free trial account will suffice, but will be limited to a small number of requests.\n",
    "- After obtaining this key, store it in plain text in your home in directory in the `~/.cohere.key` file.\n",
    "- (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.cohereai import CohereEmbedding\n",
    "from llama_index.llms import Cohere\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "Make sure other necessary items are in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.environ[\"COHERE_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "    os.environ[\"CO_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "except Exception:\n",
    "    print(f\"ERROR: You must have a Cohere API key available in your home directory at ~/.cohere.key\")\n",
    "\n",
    "# Look for the source-materials folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558afb",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation\n",
    "\n",
    "Let's start by asking the Cohere LLM a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's basic knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is very domain-specific that it won't know the answer to. A good example would an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "\"*How many Vector scholarships in AI were awarded in 2022?*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6133a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## Now send the query to Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00061d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown field: parameter model is not a valid field\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      " Vector Institute's scholarships page indicates that they offer various types of scholarships, including the Vector Artificial Intelligence Scholarship for graduate students. I am unable to provide the exact number of scholarships awarded in 2022 as this may be subject to change and the exact statistics may not be available to the public. \n",
      "\n",
      "To obtain the most up-to-date and accurate information on the number of Vector scholarships in AI awarded in 2022, I recommend reaching out to the Vector Institute directly as they would have the most current information on their scholarship programs. You can find their contact information on their website at: https://vectorinstitute.ai/contact/. \n"
     ]
    }
   ],
   "source": [
    "llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "result = llm.complete(query)\n",
    "print(f\"Result: \\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Cohere is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source materials: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pdfs\n",
    "pdf_folder_path = \"./source_documents\"\n",
    "documents = SimpleDirectoryReader(pdf_folder_path).load_data()\n",
    "print(f\"Number of source materials: {len(documents)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "## Define an embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc16fe",
   "metadata": {},
   "source": [
    "This embeddings model will convert the textual data from our PDF files into vector embeddings. These vector embeddings will later enable us to quickly find the chunk of text that most closely corresponds to our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1048c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /tmp/ttdDexSi2RiAMzk1/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "embed_model = CohereEmbedding(\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_query\"\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    "    chunk_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1690e",
   "metadata": {},
   "source": [
    "## Storage: Store the documents in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075ede5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a002db60bd43c99b39293c677811eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fca62ed5774b35a196e09e4f93ec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "## Retrieval: Now do a search to retrieve the chunk of document text that most closely matches our original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23499f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query retriever found 2 results\n",
      "First result example:\n",
      "Node ID: ba8b0272-a0cb-47ae-85d9-70fe7653c39d\n",
      "Text: 26    VECTOR SCHOLARSHIPS IN  AI ATTRACT TOP TALENT TO ONTARIO\n",
      "UNIVERSITIES  109  Vector Scholarships in AI awarded  34  Programs  13\n",
      "Universities  351  Scholarships awarded since the  program launched in\n",
      "2018 Supported with funding from the Province of Ontario, the Vector\n",
      "Institute Scholarship in Artifcial Intelligence (VSAI) helps Ontario\n",
      "uni...\n",
      "Score:  0.666\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query_retriever = index.as_retriever(service_context=service_context)\n",
    "search_query_retrieved_nodes = search_query_retriever.retrieve(query)\n",
    "print(f\"Search query retriever found {len(search_query_retrieved_nodes)} results\")\n",
    "print(f\"First result example:\\n{search_query_retrieved_nodes[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb632b45-b135-4561-9759-99fcc03e6959",
   "metadata": {},
   "source": [
    "That first result doesn't look right, but it's close? Could it be that we got the result that we wanted from that retrieval, but the results came back out of order? Let's try using a reranker to check which of our results is a closest match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea847fe",
   "metadata": {},
   "source": [
    "## Reranking: Improve the ordering of the document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dd59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CohereRerank()\n",
    "query_engine = index.as_query_engine(\n",
    "    node_postprocessors = [reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef217bc",
   "metadata": {},
   "source": [
    "## Final RAG-augmented query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63696ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I'm sorry, the query asks for the number of Vector Scholarships in AI awarded in 2022, however, the context provided only provides information about 2021-22, and there is no indication of the total number of years that the scholarships are awarded for. Therefore, I cannot answer the query based on the context provided.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(query)\n",
    "print(f\"Result: {result}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_dataloaders",
   "language": "python",
   "name": "rag_dataloaders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
