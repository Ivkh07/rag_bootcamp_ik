{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# Document Search with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LangChain](https://python.langchain.com/docs/get_started/introduction) library to run a text-generation request on open-source LLMs and embedding models using the OpenAI SDK, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "### <u>Requirements</u>\n",
    "1. As you will accessing the LLMs and embedding models through Vector AI Engineering's Kaleidoscope Service (Vector Inference + Autoscaling), you will need to request a KScope API Key:\n",
    "\n",
    "   Run the following command (replace ```<user_id>``` and ```<password>```) from **within the cluster** to obtain the API Key. The ```access_token``` in the output is your KScope API Key.\n",
    "  ```bash\n",
    "  curl -X POST -d \"grant_type=password\" -d \"username=<user_id>\" -d \"password=<password>\" https://kscope.vectorinstitute.ai/token\n",
    "  ```\n",
    "2. After obtaining the `.env` configurations, make sure to create the ```.kscope.env``` file in your home directory (```/h/<user_id>```) and set the following env variables:\n",
    "- For local models through Kaleidoscope (KScope):\n",
    "    ```bash\n",
    "    export OPENAI_BASE_URL=\"https://kscope.vectorinstitute.ai/v1\"\n",
    "    export OPENAI_API_KEY=<kscope_api_key>\n",
    "    ```\n",
    "- For OpenAI models:\n",
    "   ```bash\n",
    "   export OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n",
    "   export OPENAI_API_KEY=<openai_api_key>\n",
    "   ```\n",
    "3. (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cb87e-cdf4-4002-a700-77db86d0b318",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742aa343-c90c-4e4a-8099-a3fa218e256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a7448",
   "metadata": {},
   "source": [
    "#### Load config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e70d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root folder of the rag_bootcamp repo to PYTHONPATH\n",
    "current_dir = Path().resolve()\n",
    "parent_dir = current_dir.parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from utils.load_secrets import load_env_file\n",
    "load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e0bec6-a89c-4fca-a218-c784ec18e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\")\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "#### Set up some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "#### Make sure other necessary items are in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the source_documents folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c93ad-fe81-4acd-9ea3-eca9b425ee9a",
   "metadata": {},
   "source": [
    "#### Choose LLM and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2553d130-5b02-4852-928f-beb7ecd05d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558afb",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation\n",
    "\n",
    "Let's start by asking Llama-3.1 a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's world knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is domain-specific and it won't know the answer to. A good example would be an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "*How many Vector scholarships in AI were awarded in 2022?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6133a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## Now send the query to the open source model using KScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00061d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "I don't have access to real-time data or specific information about the number of Vector scholarships in AI awarded in 2022. For the most accurate and up-to-date information, I recommend checking directly with Vector Institute or their official website. They would have the most current details on their scholarship programs and awards.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=GENERATOR_MODEL_NAME,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    base_url=GENERATOR_BASE_URL,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "message = [\n",
    "    (\"human\", query),\n",
    "]\n",
    "try:\n",
    "    result = llm.invoke(message)\n",
    "    print(f\"Result: \\n\\n{result.content}\")\n",
    "except Exception as err:\n",
    "    if \"Error code: 503\" in err.message:\n",
    "        print(f\"The model {GENERATOR_MODEL_NAME} is not ready yet.\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Llama-3.1 is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from `source_documents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`, break them up into smaller digestible chunks, then encode them as vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cleared-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hollywood-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'11114CA Wheat Farming in Canada Industry Report.pdf'\r\n",
      "'11115CA Corn Farming in Canada Industry Report.pdf'\r\n",
      "'33639CA Auto Parts Manufacturing in Canada Industry Report.pdf'\r\n",
      "'44111CA New Car Dealers in Canada Industry Report.pdf'\r\n",
      "'48412CA Long-Distance Freight Trucking in Canada Industry Report.pdf'\r\n",
      "'48422CA Local Specialized Freight Trucking in Canada Industry Report.pdf'\r\n",
      "'48423CA Long-Distance Specialized Freight Trucking in Canada Industry Report.pdf'\r\n"
     ]
    }
   ],
   "source": [
    "ls /projects/RAG2/scotia-2/Datasets-Scotia-2/IBIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "24b42902-d145-4f61-80c2-334a4b1da886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the embeddings model...\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {'device': 'cuda', 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "print(f\"Setting up the embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=   EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source documents: 280\n",
      "Number of text chunks: 654\n",
      "CPU times: user 9.77 s, sys: 17.3 ms, total: 9.78 s\n",
      "Wall time: 9.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the IBIS pdfs\n",
    "#directory_path = \"./source_documents\"\n",
    "directory_path = \"/projects/RAG2/scotia-2/Datasets-Scotia-2/IBIS\"\n",
    "loader = PyPDFDirectoryLoader(directory_path)\n",
    "docs = loader.load()\n",
    "print(f\"Number of source documents: {len(docs)}\")\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1250, chunk_overlap=32)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Number of text chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "finished-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks2 = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "senior-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len (chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "insured-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Reuters data\n",
    "file_paths = ['/projects/RAG2/scotia-2/Datasets-Scotia-2/Agriculture_txt/agri_ca_co.csv', \n",
    "              '/projects/RAG2/scotia-2/Datasets-Scotia-2/Transport_txt/transport_CA.csv', \n",
    "              '/projects/RAG2/scotia-2/Datasets-Scotia-2/Auto_txt/auto_ca.csv', \n",
    "             ]\n",
    "def load_txt_file(file_path):\n",
    "    # Create a TextLoader instance\n",
    "\n",
    "    loader = TextLoader(file_path)\n",
    "\n",
    "    # Load the document\n",
    "\n",
    "    document = loader.load()\n",
    "    chunks2 =text_splitter.split_documents(document)\n",
    "    print(f\"Number of text chunks: {len(chunks2)}\")\n",
    "    return chunks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "literary-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 1212\n",
      "Number of text chunks: 927\n",
      "Number of text chunks: 2065\n"
     ]
    }
   ],
   "source": [
    "for file_path in file_paths:\n",
    "    chunks2 = load_txt_file(file_path)\n",
    "    chunks= chunks +chunks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-average",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef3fd15-213f-4774-88e0-bbdadf28789c",
   "metadata": {},
   "source": [
    "#### Define the embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "## Retrieval: Make the document chunks available via a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc16fe",
   "metadata": {},
   "source": [
    "The retriever will identify the document chunks that most closely match our original query. (This takes about 1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1048c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 52s, sys: 485 ms, total: 1min 52s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Retrieve the most relevant context from the vector store based on the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "other-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "industries  = [\"auto dealer\", \"wheat farming\", 'trucking']\n",
    "industry  = \"wheat farming\"\n",
    "industry =\"auto dealer\"\n",
    "query = f\"Where are most {industry} companies are located in Canada?\"\n",
    "query1  = f' Which countries are competitors for canadian {industry} companies internationally?' # bad\n",
    "\n",
    "query2 = f'What is the profit margin of Canadian {industry}'\n",
    "\n",
    "#query = f'Who are the main palyers in Canadian  {industry}?' # bad\n",
    "query3  = ' Which countries are competitors for canadian industry internationally?' # bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "neutral-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.2 ms, sys: 11.8 ms, total: 39 ms\n",
      "Wall time: 35.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "retrieved_docs = retriever.invoke(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1690e",
   "metadata": {},
   "source": [
    "Let's see what results it found. Important to note, these results are in the order the retriever thought were the best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "51dc81d7-8333-41e6-9e77-47a45ee0b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty_print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5e95e-f127-4092-b960-33b39ad21147",
   "metadata": {},
   "source": [
    "## Now send the query to the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e26d9f46-a082-4497-8ffc-9fa3eccc2ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "most new car dealers in Canada are located in Ontario. This is due to several factors, including:\n",
      "\n",
      "1. Population: Ontario contains the majority of Canada's population, contributing to above-average demand for new vehicles.\n",
      "2. Major cities: Cities like Toronto and Ottawa contribute to demand, and the region also houses the majority of automobile and SUV manufacturers, allowing dealers to leverage proximity to key suppliers to reduce transportation costs and better allocate inventories to consumer demand.\n",
      "\n",
      "Additionally, Quebec and British Columbia are also popular destinations for new car dealers in Canada, due to their relatively high population density and affluent consumers, as well as strong retail and tourism activity.\n",
      "CPU times: user 54.4 ms, sys: 7.58 ms, total: 62 ms\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rag_pipeline = RetrievalQA.from_llm(llm=llm, retriever=retriever)\n",
    "result = rag_pipeline.invoke(input=query)\n",
    "print(f\"Result: \\n\\n{result['result'].replace ('According to the provided context, ', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-bubble",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_dataloaders",
   "language": "python",
   "name": "rag_dataloaders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
