{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# Document Search with LlamaIndex using Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request on Cohere LLMs and local embedding models, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "### <u>Requirements</u>\n",
    "1. Make sure to create the ```.cohere.env``` file in your home directory (```/h/<user_id>```) and store your Cohere API key in plain text.\n",
    "2. (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ad1b0-76d3-4db8-9705-a4a4ac56ebca",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a750497-5fc0-4a7f-8ed6-931c5d8759d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.node_parser import LangchainNodeParser\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d9175-73d5-4ef0-965b-acaa3fb4a91c",
   "metadata": {},
   "source": [
    "#### Read Cohere API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be12a21-c830-4aa3-a76d-3684b9445950",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(Path.home() / \".cohere.key\", \"r\")\n",
    "    os.environ[\"COHERE_API_KEY\"] = f.read().rstrip(\"\\n\")\n",
    "    f.close()\n",
    "except Exception as err:\n",
    "    print(f\"Could not read your Cohere API key. Please make sure this is available in plain text under your home directory in ~/.cohere.key: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "#### Set up some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.text for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "#### Make sure other necessary items are in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the source_documents folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3d1c8-07cb-4e1a-88ac-087536c6e96e",
   "metadata": {},
   "source": [
    "#### Choose Cohere LLM and local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a10552-cb1a-4088-9081-05494fca9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_NAME = \"command-r\"\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558afb",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation\n",
    "\n",
    "Let's start by asking Cohere a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's world knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is domain-specific and it won't know the answer to. A good example would be an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "*How many Vector scholarships in AI were awarded in 2022?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6133a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## Now send the query to the open source model using KScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3d559a-74cf-4406-9ee4-61944f3e4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "assistant: According to the official Vector Institute website, in 2022, 40 scholarships in AI were awarded. The Vector Institute is a non-profit organization that focuses on research and talent development in the field of artificial intelligence. These scholarships, worth $10,000 each, are offered to outstanding graduate students pursuing research in AI and related fields. \n",
      "\n",
      "The Vector Institute also awarded 5 exceptional scholars with the Premier's Awards, which recognizes the top scholars among the Vector Institute scholarship recipients. These awards are named in honor of the Premier of Ontario and carry an additional cash prize of $5,000. \n",
      "\n",
      "Would you like more information on the Vector Institute or the scholarships they offer?\n"
     ]
    }
   ],
   "source": [
    "llm = Cohere(\n",
    "    model=GENERATOR_MODEL_NAME,\n",
    "    temperature=0,\n",
    "    max_tokens=128,\n",
    "    api_key=os.environ[\"COHERE_API_KEY\"],\n",
    ")\n",
    "message = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=query\n",
    "    )\n",
    "]\n",
    "\n",
    "result = llm.chat(message)\n",
    "print(f\"Result: \\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Cohere is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from `source_documents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`, break them up into smaller digestible chunks, then encode them as vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source documents: 42\n",
      "Number of text chunks: 228\n"
     ]
    }
   ],
   "source": [
    "# Load the pdfs\n",
    "directory_path = \"./source_documents\"\n",
    "docs = SimpleDirectoryReader(input_dir=directory_path).load_data()\n",
    "print(f\"Number of source documents: {len(docs)}\")\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "parser = LangchainNodeParser(RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32))\n",
    "chunks = parser.get_nodes_from_documents(docs)\n",
    "print(f\"Number of text chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "#### Define the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "268ab345-4676-4700-8965-4639751e7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the embeddings model...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setting up the embeddings model...\")\n",
    "embeddings = HuggingFaceEmbedding(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    device='cuda',\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ed121-6e4c-46e4-926c-33aa6ee77759",
   "metadata": {},
   "source": [
    "#### Set LLM and embedding model [recommended for LlamaIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7446327c-d8b9-4928-92c7-fb0af4fb0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5050576-073c-4615-9621-b6b217a13b0e",
   "metadata": {},
   "source": [
    "## Retrieval: Make the document chunks available via a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0131a2d-4cd6-4c1e-835e-540329fda2b2",
   "metadata": {},
   "source": [
    "The retriever will identify the document chunks that most closely match our original query. (This takes about 1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49d0093-0105-499a-a7e3-ebf6326a85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_model_dim(embed_model):\n",
    "    embed_out = embed_model.get_text_embedding(\"Dummy Text\")\n",
    "    return len(embed_out)\n",
    "\n",
    "faiss_dim = get_embed_model_dim(embeddings)\n",
    "faiss_index = faiss.IndexFlatL2(faiss_dim)\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(chunks, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f512cb-36f8-4afb-a8c6-0c187a0d9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# Retrieve the most relevant context from the vector store based on the query\n",
    "retrieved_docs = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68093b7f-4da7-45d8-8a58-536fb7f8aa5c",
   "metadata": {},
   "source": [
    "Let's see what results it found. Important to note, these results are in the order the retriever thought were the best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ff6d3c-b6e8-4702-8591-44e0d7b7d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "26 \n",
      "  VECTOR SCHOLARSHIPS IN \n",
      "AI ATTRACT TOP TALENT TO ONTARIO UNIVERSITIES \n",
      "109 \n",
      "Vector Scholarships in AI awarded \n",
      "34 \n",
      "Programs \n",
      "13 \n",
      "Universities \n",
      "351 \n",
      "Scholarships awarded since the \n",
      "program launched in 2018 Supported with funding from the Province of Ontario, the Vector Institute Scholarship in Artifcial Intelligence (VSAI) helps Ontario universities to attract the best and brightest students to study in AI-related master’s programs. \n",
      "Scholarship recipients connect directly with leading\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "5 \n",
      "Annual Report 2021–22 Vector Institute\n",
      "SPOTLIGHT ON FIVE YEARS OF AI \n",
      "LEADERSHIP FOR CANADIANS \n",
      "SINCE THE VECTOR INSTITUTE WAS FOUNDED IN 2017: \n",
      "2,080+ \n",
      "Students have graduated from \n",
      "Vector-recognized AI programs and \n",
      "study paths $6.2 M \n",
      "Scholarship funds committed to \n",
      "students in AI programs 3,700+ \n",
      "Postings for AI-focused jobs and \n",
      "internships ofered on Vector’s \n",
      "Digital Talent Hub $103 M \n",
      "In research funding committed to \n",
      "Vector-afliated researchers \n",
      "94 \n",
      "Research awards earned by\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "studies in a Vector-recognized AI-related master’s program or other study paths in AI — both a record number and a 27 per cent increase over the previous year. Last year also saw more than 1,000 new graduates from AI master’s programs in Ontario; a milestone achieved ahead of the province’s 2023 target. These skilled AI graduates will hold an envied role in the workforce of the future. Further, our research community has now grown to more than 700, whose infuence continues to grow; they published more than\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "23 \n",
      "RESEARCH AWARDS AND \n",
      "ACHIEVEMENTS \n",
      "Each year, members of Vector’s research community \n",
      "are recognized for outstanding contributions to AI and machine learning felds. Highlights of 2021–22 include: \n",
      "GLOBAL REACH OF VECTOR \n",
      "RESEARCHERS AND THEIR WORK \n",
      "Vector researchers published papers, gave \n",
      "presentations, or led workshops at many of the top AI conferences this year, including NeurIPS, CVPR, ICLR, ICML, and ACM FAccT. \n",
      "380+ Research papers presented at\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "and Universities \n",
      "1,007 \n",
      "graduates from recognized AI-master’s programs at Ontario universities, exceeding the province’s target to graduate 1,000 AI master’s students per year by 2023 ahead of schedule\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "## Now send the query to the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23499f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "According to the Vector Institute's Annual Report of 2021-22, 109 Vector Scholarships in AI were awarded in 2022. \n",
      "\n",
      "The Vector Institute Scholarship in Artificial Intelligence (VSAI) helps Ontario universities to attract the best students to study in AI-related master's programs.\n"
     ]
    }
   ],
   "source": [
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "result = query_engine.query(query)\n",
    "print(f\"Result: \\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb632b45-b135-4561-9759-99fcc03e6959",
   "metadata": {},
   "source": [
    "The model provides the correct answer (109) using the retrieved information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_dataloaders",
   "language": "python",
   "name": "rag_dataloaders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
